library(sqldf)
install.packages('sqldf')
library(sqldf)
setwd('~/mapping/writing/ambient_crime/twitter_data')
f <- unzip('../full_tweet_data.csv.zip')
setwd('~/mapping/writing/ambient_crime/twitter_data/contributor_bias/')
f <- unzip('../full_tweet_data.csv.zip')
d <- read.csv(f)
names(d)
users <- sqldf('SELECT User_ID, COUNT(*) AS NumMessages FROM d GROUP BY User_ID')
write.csv(users, file="userid_counts.csv") # USeful to have this as csv for excel
msg = users$NumMessages
names(users)
breaks = seq(0,max(msg), length.out=10)
breaks
max(msg)
breaks = seq(11,max(msg), length.out=10)
breaks = seq(11,max(msg), length.out=11)
msg.cut = cut(msg,, breaks, right=FALSE, left=FALSE) # (FALSE because closed left and right intervals)
breaks = seq(11,max(msg), length.out=11)
msg.cut = cut(msg, breaks, right=FALSE, left=FALSE) # (FALSE because closed left and right intervals)
msg.cut
help(cut)
summary(msg)
count(msg)
msg.freq = table(msg.cut)
msg.freq
breaks = seq(11,max(msg)+1, length.out=11)
msg.cut = cut(msg, breaks, right=FALSE, left=FALSE) # (FALSE because closed left and right intervals)
msg.cut
breaks = seq(11,max(msg)+1, length.out=11)
breaks
breaks = seq(0,max(msg)+1, length.out=11)
breaks
msg.cut = cut(msg, breaks, right=FALSE, left=FALSE) # (FALSE because closed left and right intervals)
msg.cut
msg.freq = table(msg.cut)
msg.freq
msg.relfreq = msg.freq / nrow(msg)
msg.relfreq = msg.freq / nrow(d)
msg.relfreq
options(scipen=10)
# Histogram showing skew
msg.relfreq
#pdf(file="user_frequency.pdf",width=11, height=5)
msg.relfreq
options(scipen=999)
msg.relfreq = msg.freq / nrow(users)
msg.relfreq
sum(msg.relfreq)
write.csv(msg.freq, file="freq_dist.csv") # USeful to have this as csv for excel
msg.cut
msg.relfreq
plot(msg.relfreq)
plot(msg.relfreq, type='l')
msg.cumfreq = cumsum(msg.freq)
msg.cumfreq
cumsum(msg.relfreq)
msg.cumfreq = cumsum(msg.relfreq)
msg.cumfreq
plot(msg.cumfreq, type='l')
breaks = seq(0,max(msg)+1, length.out=101)
msg.cut = cut(msg, breaks, right=FALSE, left=FALSE) # (FALSE because closed left and right intervals)
msg.freq = table(msg.cut)
write.csv(msg.freq, file="freq_dist.csv") # USeful to have this as csv for excel
# Also do cumulative relative frequency
msg.cumfreq = cumsum(msg.relfreq)
plot(msg.cumfreq, type='l')
msg.freq = table(msg.cut)
write.csv(msg.freq, file="freq_dist.csv") # USeful to have this as csv for excel
# Now turn into relative frequency distribution
msg.relfreq = msg.freq / nrow(users)
# Also do cumulative relative frequency
msg.cumfreq = cumsum(msg.relfreq)
plot(msg.cumfreq, type='l')
install.packages('ineq')
library(ineq)
library(ineq)
install.packages('ineq')
library(ineq)
help(ineq)
ineq()
install.packages('sqldf')
library(ineq)
plot(Lc(msg.cumfreq))
plot(Lc(msg))
gini = ineq(msg, type="Gini")
gini
save.image("~/mapping/writing/ambient_crime/twitter_data/contributor_bias/r_workspace.RData")
msg.relfreq
msg.freq
msg.relfreq
unlink(f) # This deletes the temporary file.
fix(msg.freq)
fix(msg.freq)
msg.freq
msg.relfreq
msg.cumfreq = cumsum(msg.relfreq)
msg.cumrelfreq = cumsum(msg.relfreq)
msg.cumrelfreq
msg.cumrelfreq
msg.relfreq=cumsum(msg.freq)
msg.relfreq
msg.cumrelfreq
msg.relfreq = msg.freq / nrow(users)
msg.relfreq
msg.cumrelfreq = cumsum(msg.relfreq)
msg.cumrelfreq
msg.cumfreq=cumsum(msg.freq)
msg.cumfreq
plot(msg.cumrelfreq)
plot(Lc(msg))
summary(Lc(msg))
Lc(msg)
lorenz.c = Lc(msg)
fix(lorenz.c)
names(lorenz.c)
lorenz.c$p
names(lorenz.c)
lorenz.c$L
names(lorenz.c)
plot(x=lorenz.c$p, y=lorenz.c$L)
plot(lorenz.c)
rm(d)
load("~/mapping/writing/ambient_crime/twitter_data/contributor_bias/r_workspace.RData")
setwd('~/mapping/writing/ambient_crime/twitter_data/contributor_bias/')
lorenz.c = Lc(msg)
library(ineq)
plot(Lc(msg))
lorenz.c = Lc(msg)
plot(lorenz.c))
plot(lorenz.c)
index = which.min((lorenz.c$p-0.9)^2)
index
names(lorenz.c)
yval = lorenx.c$L[indxex]
yval = lorenz.c$L[indxex]
val = lorenz.c$L[inddex]
val = lorenz.c$L[index]
val = lorenz.c$L[which.min((lorenz.c$p-0.9)^2)]
val
print 1 - val
echo ( 1 - val)
1 - val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.99)^2)]
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
9
val = lorenz.c$L[which.min((lorenz.c$p-0.99)^2)]
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.8)^2)]
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val
val = lorenz.c$L[which.min((lorenz.c$p-0.6)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.7)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.6)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.7)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.8)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.9)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.8)^2)]
val
1 - val # This is the proportion of tweets made by top 90% (where Lorenz curve crosses y axis at 0.9)
val = lorenz.c$L[which.min((lorenz.c$p-0.8)^2)]
val
1 - val # This is the proportion of tweets made by top 80% (where Lorenz curve crosses y axis at 0.8)
plot(lorenz.c, xlab="Cumulative proportion (p)", ylab="Proportion of all messages - L(p)")
plot(lorenz.c, xlab="Cumulative proportion - p", ylab="Proportion of all messages - L(p)")
plot(lorenz.c, xlab="Cumulative proportion: p", ylab="Proportion of all messages: L(p)")
help(Lc)
save.image("~/mapping/writing/ambient_crime/twitter_data/contributor_bias/r_workspace.RData")
setwd("~/mapping/projects/blog_kde_test/R")
image.load(kde_test.RData)
load.image(kde_test.RData)
load("~/mapping/projects/blog_kde_test/R/kde_test.RData")
plot(d1.kde)
par(mfrow=c(2,1))
plot(d1.kde)
library(ks)
library(ks)
plot(d1.kde)
d1 = read.csv("crime_2013_02.csv") # Crime data from feb 2013
d2 = read.csv("crime_2013_03.csv") # Crime data from march 2013
#plot (x=d1$EASTING, y=d1$NORTHING, type='p')
#points(x=d2$EASTING, y=d2$NORTHING, type='p', col='blue')
# Temporarily only use first 10,000 points
#d1 <- d1[1:1000,]
#d2 <- d2[1:1000,]
d1.kde <- kde(x=d1[,2:3]) # (ignoring first colum which has object ID)
d2.kde <- kde(x=d2[,2:3]) # (ignoring first colum which has object ID)
par(mfrow=c(2,1))
plot(d1.kde)
plot(d2.kde)
par(cex=0.5)
quartz()
par(cex=0.5)
par(mfrow=c(2,1))
par(cex=0.5)
plot(d1.kde )
plot(d2.kde)
test.check <- kde.test( x1=d1, x2=d1) # Sanity check - test two identical kdes
par(mfrow=c(1,1))
par(cex=0.5)
pdf("kde_2013_02.pdf")
plot(d1.kde, main="Feb 2013")
pdf("kde_2013_02.pdf")
plot(d1.kde, main="Feb 2013", width=18, height=15)
dev.off()
pdf("kde_2013_03.pdf")
plot(d2.kde, main="Mar 2013", width=18, height=15)
dev.off()
test.check <- kde.test( x1=d1, x2=d1) # Sanity check - test two identical kdes
test <- kde.test( x1=d1, x2=d2) # The proper test
save.image("~/mapping/projects/blog_kde_test/R/kde_test.RData")
test.check
test
